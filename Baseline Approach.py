# -*- coding: utf-8 -*-
"""RAW ADNI DATA (EFFNET).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10KN7-ILSMGLZ2F5jHdH_e2Qrq2oGX1wL
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import files"""

import os
import json
import nibabel as nib
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import Sequential, layers
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import seaborn as sns
import numpy as np

def find_nifti_files(directory, subject_folder):
    """ Recursively search for NIfTI files in a directory. """
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file == f'{subject_folder}.nii':
                return os.path.join(root, file)
    return None

root_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/DATABASE/!THESIS! - EXP. 1 (.nii)'
data = []

for class_name in os.listdir(root_dir):
    class_path = os.path.join(root_dir, class_name)
    if os.path.isdir(class_path):
        for subject_folder in os.listdir(class_path):
            subject_path = os.path.join(class_path, subject_folder)
            image_path = find_nifti_files(subject_path, subject_folder)  # Find the NIfTI file recursively

            if image_path and os.path.exists(image_path):
                data.append({'filename': image_path, 'label': class_name})
            else:
                print(f"Expected image file not found for subject {subject_folder} in class {class_name}")

# Create a DataFrame from the collected data
if data:
    data_df = pd.DataFrame(data)

    # Analyse the distribution of classes among the images
    if 'label' in data_df.columns:
        class_distribution = data_df['label'].value_counts()

        plt.figure(figsize=(10, 6))
        class_distribution.plot(kind='bar')
        plt.title('Class Distribution')
        plt.xlabel('Class')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.show()
    else:
        print("No 'label' column found in DataFrame.")
else:
    print("No data collected, check the directory paths and file names.")

"""## Check distribution for Classes"""

class_distribution = data_df['label'].value_counts()

plt.figure(figsize=(10, 6))
class_distribution.plot(kind='bar')
plt.title('Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks(rotation=45)  # Rotate x-labels for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""## Split data into Training, test and validation databases"""

file_paths = []
labels = []
for class_name in ['AD', 'CN', 'EMCI', 'LMCI', 'MCI']:
    class_path = os.path.join(root_dir, class_name)
    for subject_id in os.listdir(class_path):
        subject_path = os.path.join(class_path, subject_id)
        nifti_file = f"{subject_id}.nii"
        file_paths.append(os.path.join(subject_path, nifti_file))
        labels.append(class_name)

data_df = pd.DataFrame({'filename': file_paths, 'label': labels})

train_df, test_val_df = train_test_split(data_df, test_size=0.3, random_state=42, stratify=data_df['label'])
val_df, test_df = train_test_split(test_val_df, test_size=(0.05 / 0.3), random_state=42, stratify=test_val_df['label'])

"""# Align with MNI template and extract middle slice

See the seccond experiment for reasoining
"""

!pip install antspyx

import ants

atlas_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/MNI_Templates/MNI_Template/MNI152_T1_1mm.nii'
atlas = ants.image_read(atlas_path)

base_subjects_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/DATABASE/!THESIS! - EXP. 1 (.nii)'

output_base_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/Exp 1_Middle slices'
os.makedirs(output_base_dir, exist_ok=True)

def register_and_extract_slice(subject_path, atlas_img):
    try:
        subject_img = ants.image_read(subject_path)
        print(f"Registering {subject_path} to atlas...")

        registration = ants.registration(fixed=atlas_img, moving=subject_img, type_of_transform='SyN')
        warped_image = registration['warpedmovout']

        middle_slice_index = warped_image.shape[2] // 2
        axial_slice = warped_image.numpy()[:, :, middle_slice_index]

        return axial_slice, middle_slice_index
    except Exception as e:
        print(f"An error occurred during the registration of {subject_path}: {e}")
        return None, None

def save_slice_as_png(slice_data, output_path):
    plt.imsave(output_path, np.rot90(slice_data), cmap='gray')

def process_nested_directories(current_dir, atlas_img, class_name=None):
    for entry in os.listdir(current_dir):
        path = os.path.join(current_dir, entry)
        if os.path.isdir(path):
            # Use the directory name as the class name if this is the first level under the base subjects directory
            new_class_name = entry if current_dir == base_subjects_dir else class_name
            process_nested_directories(path, atlas_img, new_class_name)
        elif path.endswith('.nii') or path.endswith('.nii.gz'):
            print(f"Processing NIfTI file: {path}")
            slice, index = register_and_extract_slice(path, atlas_img)
            if slice is not None:
                slice_filename = f'middle_slice_index_{index}_{os.path.basename(entry)}.png'
                class_output_dir = os.path.join(output_base_dir, class_name if class_name else 'Unknown')
                os.makedirs(class_output_dir, exist_ok=True)
                slice_output_path = os.path.join(class_output_dir, slice_filename)
                save_slice_as_png(slice, slice_output_path)

# Start processing from the base subjects directory
process_nested_directories(base_subjects_dir, atlas)

"""## Handling errors regarding outliers from the datacollection

# Transform from Grayscale to RGB to match the Imagenet
"""

from PIL import Image

input_directory = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/Exp 1_Middle slices'
output_directory = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/RGB'

if not os.path.exists(output_directory):
    os.makedirs(output_directory)

for filename in os.listdir(input_directory):
    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
        file_path = os.path.join(input_directory, filename)
        grayscale_image = Image.open(file_path)

        rgb_image = grayscale_image.convert("RGB")
        output_file_path = os.path.join(output_directory, filename)
        rgb_image.save(output_file_path)

print("All images have been converted to RGB and saved to the output directory.")

input_directory = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/Exp 1_Middle slices'
output_directory = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/RGB'

if not os.path.exists(output_directory):
    os.makedirs(output_directory)
def convert_images(input_dir, output_dir):
    for root, dirs, files in os.walk(input_dir):
        for filename in files:
            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                file_path = os.path.join(root, filename)
                grayscale_image = Image.open(file_path)
                rgb_image = grayscale_image.convert("RGB")
                relative_path = os.path.relpath(root, input_dir)
                output_subfolder = os.path.join(output_dir, relative_path)

                if not os.path.exists(output_subfolder):
                    os.makedirs(output_subfolder)

                output_file_path = os.path.join(output_subfolder, filename)
                rgb_image.save(output_file_path)

convert_images(input_directory, output_directory)

print("All images have been converted to RGB and saved to the output directory.")

"""# Make a dataframe (csv)"""

images_base_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/RGB'
json_base_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/DATABASE/!THESIS! - (before "recon-all")'

data = []

def load_json_labels(json_path):
    with open(json_path, 'r') as json_file:
        return json.load(json_file)

for class_folder in os.listdir(json_base_dir):
    class_folder_path = os.path.join(json_base_dir, class_folder)
    if not os.path.isdir(class_folder_path):
        continue

    for subject_folder in os.listdir(class_folder_path):
        subject_folder_path = os.path.join(class_folder_path, subject_folder)
        if not os.path.isdir(subject_folder_path):
            continue

        subject_id = subject_folder
        json_filename = f"{subject_id}.json"
        json_path = os.path.join(subject_folder_path, json_filename)
        if not os.path.isfile(json_path):
            continue

        labels = load_json_labels(json_path)
        image_filename = f"middle_slice_index_91_{subject_id}.nii.png"
        image_path = os.path.join(images_base_dir, image_filename)

        if not os.path.isfile(image_path):
            continue

        data.append({
            'image_path': image_path,
            'Subject_ID': subject_id,
            'Sex': labels.get('Sex', ''),
            'Class': class_folder,  # Using the folder name as the class
            'Age': labels.get('Age', '')
        })

# Check if we collected any data
if data:
    df = pd.DataFrame(data)
    output_csv_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/CSV/RGB.csv'
    df.to_csv(output_csv_path, index=False)
    print(f"DataFrame created with {len(df)} entries. Saved to {output_csv_path}")
    print(df.head())
else:
    print("No data was collected. Please check the directories and file names.")

images_base_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/RGB'

data = []
for class_folder in os.listdir(images_base_dir):
    class_folder_path = os.path.join(images_base_dir, class_folder)
    if not os.path.isdir(class_folder_path):
        continue  # Skip files, process directories only

    for image_file in os.listdir(class_folder_path):
        image_path = os.path.join(class_folder_path, image_file)
        if not image_file.lower().endswith('.png'):  # Check for PNG images
            continue
        data.append({
            'image_path': image_path,
            'Class': class_folder  # Using the folder name as the class label
        })

# Check if we collected any data
if data:
    df = pd.DataFrame(data)
    output_csv_path = '/xd'
    df.to_csv(output_csv_path, index=False)
    print(f"DataFrame created with {len(df)} entries. Saved to {output_csv_path}")
    print(df.head())
else:
    print("No data was collected. Please check the directories and file names.")

"""## Split"""

df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/RGB.csv')
train_data, test_data = train_test_split(df, test_size=0.2, stratify=df['Class'], random_state=42)

# Then, split the remaining 80% into training and validation sets
train_data, val_data = train_test_split(train_data, test_size=0.125, stratify=train_data['Class'], random_state=42)
# test_size = 0.125 to ensure validation is 10% of the original data (0.125 * 0.8 = 0.1)

# Save the datasets to CSV
train_data.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/CSV/SPLIT/train_data.csv', index=False)
val_data.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/CSV/SPLIT/val_data.csv', index=False)
test_data.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/CSV/SPLIT/test_data.csv', index=False)

train_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/CSV/SPLIT/train_data.csv')
val_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/CSV/SPLIT/val_data.csv')
test_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/CSV/SPLIT/test_data.csv')

# Check the distribution of classes in the training set
print("Training Data Distribution:")
print(train_data['Class'].value_counts())

# Check the distribution of classes in the validation set
print("\nValidation Data Distribution:")
print(val_data['Class'].value_counts())

# Check the distribution of classes in the test set
print("\nTest Data Distribution:")
print(test_data['Class'].value_counts())

"""# Data augmentation"""

import cv2
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/CSV/SPLIT/train_data.csv')

augmented_data_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Augmented pictures'
os.makedirs(augmented_data_path, exist_ok=True)

def augment_data(file_path, n_generated_samples, save_to_dir, class_label):
    data_gen = ImageDataGenerator(rotation_range=10,
                                  width_shift_range=0.1,
                                  height_shift_range=0.1,
                                  horizontal_flip=True,
                                  vertical_flip=True,
                                  fill_mode='nearest')

    image = cv2.imread(file_path)
    image = image.reshape((1,) + image.shape)
    file_name = os.path.basename(file_path).split('.')[0]
    save_prefix = 'aug_' + file_name

    augmented_info = []

    for i, batch in enumerate(data_gen.flow(image, batch_size=1, save_to_dir=save_to_dir, save_prefix=save_prefix, save_format='png')):
        if i >= n_generated_samples:
            break
        aug_file_path = os.path.join(save_to_dir, save_prefix + f'_{i}.png')
        augmented_info.append({'image_path': aug_file_path, 'Class': class_label})

    return augmented_info

augmented_data_info = []
for index, row in train_df.iterrows():
    img_path = row['image_path']
    class_label = row['Class']
    class_dir = os.path.join(augmented_data_path, class_label)
    os.makedirs(class_dir, exist_ok=True)

    augmented_data_info += augment_data(img_path, n_generated_samples=5, save_to_dir=class_dir, class_label=class_label)

augmented_df = pd.DataFrame(augmented_data_info)
augmented_df.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/augment.csv', index=False)

print("Data augmentation complete.")

base_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Augmented pictures'

data = []

for label in os.listdir(base_dir):
    class_dir = os.path.join(base_dir, label)


    if os.path.isdir(class_dir):
        for file_name in os.listdir(class_dir):
            img_path = os.path.join(class_dir, file_name)
            if os.path.isfile(img_path) and file_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                data.append({'image_path': img_path, 'Class': label})

df = pd.DataFrame(data)

# Save
output_csv = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/augment.csv'
df.to_csv(output_csv, index=False)

print(f"New CSV saved to {output_csv}")

import random
import matplotlib.image as mpimg

df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/augment.csv')

random_rows = df.sample(n=5)

for index, row in random_rows.iterrows():
    image_path = row['image_path']
    label = row['Class']

    # Load and display
    img = mpimg.imread(image_path)
    plt.imshow(img)
    plt.title('Label: ' + str(label))
    plt.axis('off')
    plt.show()

"""# Constructing the model



"""

import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import Sequential, layers
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

augmented_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/augment.csv')
train_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/CSV/SPLIT/train_data.csv')
val_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/CSV/SPLIT/val_data.csv')
test_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/Exp 1/CSV/SPLIT/test_data.csv')


combined_train_data = pd.concat([train_df, augmented_df])

train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_dataframe(
    dataframe=combined_train_data,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_dataframe(
    dataframe=val_data,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

test_generator = test_datagen.flow_from_dataframe(
    dataframe=test_df,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Custom model
custom_model = Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.5),
    layers.Flatten(),
    layers.Dropout(0.3),
    layers.Dense(128, activation='elu'),
    layers.Dense(5, activation='softmax')
])

custom_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 50
validation_steps = 50

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)

history = custom_model.fit(train_generator, epochs=epochs, validation_data=val_generator, callbacks=[early_stopping])

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

## Test the model on unseen data to understand how the model works
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_true = test_generator.classes
y_pred = custom_model.predict(test_generator)
y_pred_classes = y_pred.argmax(axis=1)

# classification report
print(classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys()))

# confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()
