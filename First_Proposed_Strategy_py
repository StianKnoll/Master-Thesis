# -*- coding: utf-8 -*-
"""Exp 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16k2TJxQUfheHHqi9MQL4yyMTuWJMltSV
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import liberaries"""

!pip install nibabel

!pip install nilearn

import os
import numpy as np
import nibabel as nib
from nilearn import datasets, image
import matplotlib.pyplot as plt

"""# Image visualizations

We can see that the middle slize should be 128. But this slice would not work in a model because it does not show anything.

We will check ITKSnap to see where the middle slice is:

The middle slice for this Photo is 105. There are differences in the middle slice for our database, we therefore need to align with MNI template
"""

def load_nifti_file(file_path):
    img = nib.load(file_path)
    data = img.get_fdata()
    return data

file_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/DATABASE/!THESIS! INT. NORM. (NO SS)/AD/007_S_4272/007_S_4272_orig.nii'
nifti_array = load_nifti_file(file_path)
print("NIFTI data shape:", nifti_array.shape)

# Calculate the middle slice index for the axial view
middle_slice_index = nifti_array.shape[1] // 2

# Extract five middle slices from the axial view
middle_slices = nifti_array[:, middle_slice_index - 2:middle_slice_index + 3, :]

# Visualizing the middle slices
fig, axes = plt.subplots(1, 5, figsize=(15, 3))
for i, ax in enumerate(axes):
    ax.imshow(middle_slices[:, i, :], cmap='gray')
    ax.title.set_text(f'Slice {middle_slice_index - 2 + i}')
    ax.axis('off')  # Turn off axis numbering
plt.tight_layout()
plt.show()

"""As we can see the middle slice calculated does not show hippocampus nor enough brain structure to see which class it is

We find middle slice of the image using ITKSNAP visualisation tool
"""

import nibabel as nib
import matplotlib.pyplot as plt

# Load the NIFTI file
file_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/DATABASE/!THESIS! INT. NORM. (NO SS)/AD/007_S_4272/007_S_4272_orig.nii'
nifti_img = nib.load(file_path)

# Extract the data from the NIFTI file
nifti_data = nifti_img.get_fdata()

# Assuming LIA orientation and the second index represents the axial plane
# The slice_index is taken from the second dimension
slice_index = 105  # Example index, adjust based on ITK-Snap coordinates

# Extract the specified axial slice
axial_slice = nifti_data[:, slice_index, :]


plt.imshow(np.flipud(axial_slice.T), cmap='gray')
plt.axis('off')
plt.title(f'Axial Slice at Index {slice_index}')
plt.show()

"""# Align images with MNI template

We will use the Antsypyx package to align our images with the mni template to make sure that all images are in the same standard, and show all information

Runtime - 4 Hours 47 minutes
"""

!pip install antspyx

import ants
import os
import numpy as np
import matplotlib.pyplot as plt

atlas_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/MNI_Templates/MNI_Template/MNI152_T1_1mm.nii'
atlas = ants.image_read(atlas_path)

base_subjects_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/DATABASE/!THESIS! INT. NORM. (NO SS)'
output_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2'
os.makedirs(output_dir, exist_ok=True)


def register_and_extract_slice(subject_path, atlas_img):
    try:

        subject_img = ants.image_read(subject_path)
        print(f"Registering {subject_path} to atlas...")

        registration = ants.registration(fixed=atlas_img, moving=subject_img, type_of_transform='SyN')
        warped_image = registration['warpedmovout']

        # Extract the middle axial slice index in atlas space
        middle_slice_index = warped_image.shape[2] // 2
        axial_slice = warped_image.numpy()[:, :, middle_slice_index]

        return axial_slice, middle_slice_index
    except Exception as e:
        print(f"An error occurred during the registration of {subject_path}: {e}")
        return None, None

def save_slice_as_png(slice_data, output_path):
    plt.imsave(output_path, np.rot90(slice_data), cmap='gray')

# Recursive function to process all NIfTI files within the nested directory structure
def process_nested_directories(current_dir, atlas_img):
    for entry in os.listdir(current_dir):
        path = os.path.join(current_dir, entry)
        if os.path.isdir(path):
            process_nested_directories(path, atlas_img)
        elif path.endswith('.nii') or path.endswith('.nii.gz'):
            print(f"Processing NIfTI file: {path}")
            slice, index = register_and_extract_slice(path, atlas_img)
            if slice is not None:
                slice_filename = f'middle_slice_index_{index}_{os.path.basename(entry)}.png'
                slice_output_path = os.path.join(output_dir, slice_filename)
                save_slice_as_png(slice, slice_output_path)

process_nested_directories(base_subjects_dir, atlas)

"""## Visualize new middle slice"""

import cv2

img = cv2.imread('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Dataframe grayscale/middle_slice_index_91_003_S_4350_orig.nii.png', cv2.IMREAD_GRAYSCALE)

if img is not None:
    plt.figure(figsize=(8, 8))
    plt.imshow(img, cmap='gray')
    plt.title('MRI Image Visualization')
    plt.axis('off')
    plt.show()
else:
    print("Error loading the image. Please check the file path.")

"""# Transform from Grayscale (1) to RGB (3) since the Efficientnet V2S handles RGB pictures

This is key since we will load the basemodel on imagenet
"""

from PIL import Image

input_directory = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Dataframe grayscale'
output_directory = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Dataframe RGB'

if not os.path.exists(output_directory):
    os.makedirs(output_directory)

for filename in os.listdir(input_directory):
    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
        file_path = os.path.join(input_directory, filename)
        grayscale_image = Image.open(file_path)

        rgb_image = grayscale_image.convert("RGB")
        output_file_path = os.path.join(output_directory, filename)
        rgb_image.save(output_file_path)

print("All images have been converted to RGB and saved to the output directory.")

"""# Create a CSV dataframe for the pictures with labels"""

import json
import pandas as pd

images_base_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Dataframe RGB'
json_base_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/DATABASE/!THESIS! - (after "recon-all" and .mgz --> .nii)'

data = []

def load_json_labels(json_path):
    with open(json_path, 'r') as json_file:
        return json.load(json_file)

for class_folder in os.listdir(json_base_dir):
    class_folder_path = os.path.join(json_base_dir, class_folder)
    if not os.path.isdir(class_folder_path):
        continue  # Skip files, process directories only

    for subject_folder in os.listdir(class_folder_path):
        subject_folder_path = os.path.join(class_folder_path, subject_folder)
        if not os.path.isdir(subject_folder_path):
            continue  # Skip files

        subject_id = subject_folder
        json_filename = f"{subject_id}.json"
        json_path = os.path.join(subject_folder_path, json_filename)
        if not os.path.isfile(json_path):
            continue

        labels = load_json_labels(json_path)
        image_filename = f"middle_slice_index_91_{subject_id}_orig.nii.png"
        image_path = os.path.join(images_base_dir, image_filename)

        if not os.path.isfile(image_path):
            continue

        data.append({
            'image_path': image_path,
            'Subject_ID': subject_id,
            'Sex': labels.get('Sex', ''),
            'Class': class_folder,  # Using the folder name as the class
            'Age': labels.get('Age', '')
        })

# Check if we collected any data
if data:
    df = pd.DataFrame(data)
    output_csv_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/rgb.csv'
    df.to_csv(output_csv_path, index=False)
    print(f"DataFrame created with {len(df)} entries. Saved to {output_csv_path}")
    print(df.head())
else:
    print("No data was collected. Please check the directories and file names.")

"""# Split

We will split our training into 70/20/10 with 20% for test and 10% for validation
"""

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/rgb.csv')

train_data, temp_data = train_test_split(df, test_size=0.2, stratify=df['Class'], random_state=42)

train_data, val_data = train_test_split(train_data, test_size=0.125, stratify=train_data['Class'], random_state=42)
# validation = 0.125 to ensure validation is 10% of the original data (0.125 * 0.8 = 0.1)

train_data.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/train_data.csv', index=False)
val_data.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/val_data.csv', index=False)
temp_data.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/test_data.csv', index=False)

import pandas as pd

train_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/train_data.csv')
test_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/test_data.csv')
val_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/val_data.csv')

# Check the distribution of classes in the training set
print("Training Data Distribution:")
print(train_data['Class'].value_counts())

# Check the distribution of classes in the validation set
print("\nValidation Data Distribution:")
print(val_data['Class'].value_counts())

# Check the distribution of classes in the test set
print("\nTest Data Distribution:")
print(test_data['Class'].value_counts())

"""# Augment the data on training set"""

import os
import pandas as pd
import cv2
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/train_data.csv')

augmented_data_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test augmented'
os.makedirs(augmented_data_path, exist_ok=True)

def augment_data(file_path, n_generated_samples, save_to_dir, class_label):
    data_gen = ImageDataGenerator(rotation_range=10,
                                  width_shift_range=0.1,
                                  height_shift_range=0.1,
                                  horizontal_flip=True,
                                  vertical_flip=True,
                                  fill_mode='nearest')

    image = cv2.imread(file_path)
    image = image.reshape((1,) + image.shape)
    file_name = os.path.basename(file_path).split('.')[0]
    save_prefix = 'aug_' + file_name

    augmented_info = []

    for i, batch in enumerate(data_gen.flow(image, batch_size=1, save_to_dir=save_to_dir, save_prefix=save_prefix, save_format='png')):
        if i >= n_generated_samples:
            break
        aug_file_path = os.path.join(save_to_dir, save_prefix + f'_{i}.png')
        augmented_info.append({'image_path': aug_file_path, 'Class': class_label})

    return augmented_info

augmented_data_info = []
for index, row in train_df.iterrows():
    img_path = row['image_path']
    class_label = row['Class']
    class_dir = os.path.join(augmented_data_path, class_label)
    os.makedirs(class_dir, exist_ok=True)

    augmented_data_info += augment_data(img_path, n_generated_samples=5, save_to_dir=class_dir, class_label=class_label)

# Create a DataFrame for the augmented data and save it
augmented_df = pd.DataFrame(augmented_data_info)
augmented_df.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test augmented_augmented_data.csv', index=False)

print("Data augmentation complete.")

"""### Couldnt open the pictures from the CSV. Making a new CSV with error handling"""

import os
import pandas as pd

base_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test augmented'

data = []

for label in os.listdir(base_dir):
    class_dir = os.path.join(base_dir, label)


    if os.path.isdir(class_dir):
        for file_name in os.listdir(class_dir):
            img_path = os.path.join(class_dir, file_name)
            if os.path.isfile(img_path) and file_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                data.append({'image_path': img_path, 'Class': label})

df = pd.DataFrame(data)

# Save
output_csv = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test augmented_augmented_data.csv'
df.to_csv(output_csv, index=False)

print(f"New CSV saved to {output_csv}")

import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test augmented_augmented_data.csv')

random_rows = df.sample(n=5)

for index, row in random_rows.iterrows():
    image_path = row['image_path']
    label = row['Class']

    img = mpimg.imread(image_path)
    plt.imshow(img)
    plt.title('Label: ' + str(label))
    plt.axis('off')
    plt.show()





"""# Construct the model with test and validation

Build model to not create bias for any strategy: Combine EfficientNetV2S with an easy custom model with few layers.

Use 1/255 normalisation (Should not do anything on the ones that allready are between 0, 1)
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import Sequential, layers
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt


augmented_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/augmented_data.csv')
train_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/train_data.csv')
test_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/test_data.csv')
val_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/val_data.csv')
combined_train_data = pd.concat([train_df, augmented_df])

train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_dataframe(
    dataframe=combined_train_data,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_dataframe(
    dataframe=val_data,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

test_generator = test_datagen.flow_from_dataframe(
    dataframe=test_df,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

custom_model = Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.5),
    layers.Flatten(),
    layers.Dropout(0.3),
    layers.Dense(128, activation='elu'),
    layers.Dense(5, activation='softmax')
])

custom_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 50
validation_steps = 50

early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)

history = custom_model.fit(train_generator, epochs=epochs, validation_data=val_generator, callbacks=[early_stopping])

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()


plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

## Test the model on unseen data to understand how the model works
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_true = test_generator.classes
y_pred = custom_model.predict(test_generator)
y_pred_classes = y_pred.argmax(axis=1)

# classification report
print(classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys()))

# confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()

"""## We can see that there are problems with the classification. Can it be because there are so minor changes from CN-EMCI-MCI-LMCI-AD.

We will test by making a new model that only classify 3 stages (AD, MCI, CN)
"""

import pandas as pd

augmented_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/augmented_data.csv')
train_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/train_data.csv')
test_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/test_data.csv')
val_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/val_data.csv')

# Filter the dataframes to only include 'AD', 'CN', and 'MCI'
train_filtered = train_df[train_df['Class'].isin(['AD', 'CN', 'MCI'])]
val_filtered = val_df[val_df['Class'].isin(['AD', 'CN', 'MCI'])]
test_filtered = test_df[test_df['Class'].isin(['AD', 'CN', 'MCI'])]
augmented_filtered = augmented_df[augmented_df['Class'].isin(['AD', 'CN', 'MCI'])]

# Save the filtered datasets to CSV files
train_filtered.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/Test 3 stage class/train_AD_CN_MCI.csv', index=False)
val_filtered.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/Test 3 stage class/val_AD_CN_MCI.csv', index=False)
test_filtered.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/Test 3 stage class/test_AD_CN_MCI.csv', index=False)
augmented_filtered.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/Test 3 stage class/augmented_AD_CN_MCI.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import Sequential, layers
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt


augmented_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/Test 3 stage class/augmented_AD_CN_MCI.csv')
train_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/Test 3 stage class/train_AD_CN_MCI.csv')
test_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/Test 3 stage class/test_AD_CN_MCI.csv')
val_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 2 SOTA/Test/Test 3 stage class/val_AD_CN_MCI.csv')

combined_train_data = pd.concat([train_df, augmented_df])

train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_dataframe(
    dataframe=combined_train_data,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_dataframe(
    dataframe=val_data,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

test_generator = test_datagen.flow_from_dataframe(
    dataframe=test_df,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

custom_model = Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.5),
    layers.Flatten(),
    layers.Dropout(0.3),
    layers.Dense(128, activation='elu'),
    layers.Dense(3, activation='softmax')
])

custom_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 50
validation_steps = 50

early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)

history = custom_model.fit(train_generator, epochs=epochs, validation_data=val_generator, callbacks=[early_stopping])

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

## Test the model on unseen data to understand how the model works
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_true = test_generator.classes
y_pred = custom_model.predict(test_generator)
y_pred_classes = y_pred.argmax(axis=1)

# classification report
print(classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys()))

# confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()



"""# Report on the experiment

For result part: We can see that the model does better on 3 classes instead of 5, but the mdodel is still overfitting

For discussion: Why does it overfit? Is the dataset to small? Can we test it?

The dataset can not be tested with more data in hence to the processing time for each of the strategies.
"""

