# -*- coding: utf-8 -*-
"""Exp 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BjX2X4hqtf5TFOT1dn-uTh4hTdA6jLBQ
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import libaries"""

!pip install nibabel

!pip install nilearn

import os
import numpy as np
import nibabel as nib
from nilearn import datasets, image
import matplotlib.pyplot as plt



"""# Image Visualization"""

def load_nifti_file(file_path):
    img = nib.load(file_path)
    data = img.get_fdata()
    return data

file_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/DATABASE/!THESIS! INT. NORM. (SS)/AD/006_S_4713/006_S_4713_brainmask.nii'

nifti_array = load_nifti_file(file_path)
print("NIFTI data shape:", nifti_array.shape)

middle_slice_index = nifti_array.shape[1] // 2
middle_slices = nifti_array[:, middle_slice_index - 2:middle_slice_index + 3, :]


fig, axes = plt.subplots(1, 5, figsize=(15, 3))
for i, ax in enumerate(axes):
    ax.imshow(middle_slices[:, i, :], cmap='gray')
    ax.title.set_text(f'Slice {middle_slice_index - 2 + i}')
    ax.axis('off')  # Turn off axis numbering
plt.tight_layout()
plt.show()



"""# Align with MNI template:

As we see in the exploration earlier we also need to align with MNI template here

Runtime: 4H
"""

import ants
import os
import numpy as np
import matplotlib.pyplot as plt


atlas_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/MNI_Templates/MNI_Template/MNI152_T1_1mm.nii'
atlas = ants.image_read(atlas_path)

base_subjects_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/DATABASE/!THESIS! INT. NORM. (SS)'
output_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Grayscale Pictures'
os.makedirs(output_dir, exist_ok=True)


def register_and_extract_slice(subject_path, atlas_img):
    try:
        subject_img = ants.image_read(subject_path)
        print(f"Registering {subject_path} to atlas...")
        registration = ants.registration(fixed=atlas_img, moving=subject_img, type_of_transform='SyN')
        warped_image = registration['warpedmovout']

        middle_slice_index = warped_image.shape[2] // 2
        axial_slice = warped_image.numpy()[:, :, middle_slice_index]

        return axial_slice, middle_slice_index
    except Exception as e:
        print(f"An error occurred during the registration of {subject_path}: {e}")
        return None, None


def save_slice_as_png(slice_data, output_path):
    plt.imsave(output_path, np.rot90(slice_data), cmap='gray')

def process_nested_directories(current_dir, atlas_img):
    for entry in os.listdir(current_dir):
        path = os.path.join(current_dir, entry)
        if os.path.isdir(path):
            process_nested_directories(path, atlas_img)
        elif path.endswith('.nii') or path.endswith('.nii.gz'):
            print(f"Processing NIfTI file: {path}")
            slice, index = register_and_extract_slice(path, atlas_img)
            if slice is not None:

                slice_filename = f'middle_slice_index_{index}_{os.path.basename(entry)}.png'
                slice_output_path = os.path.join(output_dir, slice_filename)
                save_slice_as_png(slice, slice_output_path)

# Start processing from the base subjects directory
process_nested_directories(base_subjects_dir, atlas)

"""# Converting the Grayscale over to RGB"""

import os
from PIL import Image

input_directory = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Grayscale Pictures'
output_directory = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/RGB pictures'

if not os.path.exists(output_directory):
    os.makedirs(output_directory)

for filename in os.listdir(input_directory):
    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
        file_path = os.path.join(input_directory, filename)
        grayscale_image = Image.open(file_path)

        rgb_image = grayscale_image.convert("RGB")

        output_file_path = os.path.join(output_directory, filename)
        rgb_image.save(output_file_path)

print("All images have been converted to RGB and saved to the output directory.")

from PIL import Image
from IPython.display import display


file_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/RGB pictures/middle_slice_index_91_003_S_4441_brainmask.nii.png'

grayscale_image = Image.open(file_path)

print(f"Original image info:")
print(f"Mode (color format): {grayscale_image.mode}")
print(f"Size (width x height): {grayscale_image.size}")
print(f"Data type: {type(grayscale_image)}")

if grayscale_image.mode != "RGB":
    rgb_image = grayscale_image.convert("RGB")
else:
    rgb_image = grayscale_image

print("\nConverted image info:")
print(f"Mode (color format): {rgb_image.mode}")
print(f"Size (width x height): {rgb_image.size}")
print(f"Data type: {type(rgb_image)}")

# Show the image
display(rgb_image)



"""# Split"""

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/CSV.csv')

train_data, test_data = train_test_split(df, test_size=0.2, stratify=df['Class'], random_state=42)

train_data, val_data = train_test_split(train_data, test_size=0.125, stratify=train_data['Class'], random_state=42)
# test_size = 0.125 to ensure validation is 10% of the original data (0.125 * 0.8 = 0.1)

train_data.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/train_data.csv', index=False)
val_data.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/val_data.csv', index=False)
test_data.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/test_data.csv', index=False)

import pandas as pd

train_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/train_data.csv')
test_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/val_data.csv')
val_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/val_data.csv')

# Check the distribution of classes in the training set
print("Training Data Distribution:")
print(train_data['Class'].value_counts())

# Check the distribution of classes in the validation set
print("\nValidation Data Distribution:")
print(val_data['Class'].value_counts())

# Check the distribution of classes in the test set
print("\nTest Data Distribution:")
print(test_data['Class'].value_counts())

"""# Data augmentation"""

import os
import pandas as pd
import cv2
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/train_data.csv')

augmented_data_path = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/Augmented'
os.makedirs(augmented_data_path, exist_ok=True)

def augment_data(file_path, n_generated_samples, save_to_dir, class_label):
    data_gen = ImageDataGenerator(rotation_range=10,
                                  width_shift_range=0.1,
                                  height_shift_range=0.1,
                                  horizontal_flip=True,
                                  vertical_flip=True,
                                  fill_mode='nearest')

    image = cv2.imread(file_path)
    image = image.reshape((1,) + image.shape)
    file_name = os.path.basename(file_path).split('.')[0]
    save_prefix = 'aug_' + file_name

    augmented_info = []

    for i, batch in enumerate(data_gen.flow(image, batch_size=1, save_to_dir=save_to_dir, save_prefix=save_prefix, save_format='png')):
        if i >= n_generated_samples:
            break
        aug_file_path = os.path.join(save_to_dir, save_prefix + f'_{i}.png')
        augmented_info.append({'image_path': aug_file_path, 'Class': class_label})

    return augmented_info

augmented_data_info = []
for index, row in train_df.iterrows():
    img_path = row['image_path']
    class_label = row['Class']
    class_dir = os.path.join(augmented_data_path, class_label)
    os.makedirs(class_dir, exist_ok=True)

    augmented_data_info += augment_data(img_path, n_generated_samples=5, save_to_dir=class_dir, class_label=class_label)

# Create a DataFrame for the augmented data and save it
augmented_df = pd.DataFrame(augmented_data_info)
augmented_df.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test', index=False)

print("Data augmentation complete.")

import os
import pandas as pd

base_dir = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Augmented images'

data = []

for label in os.listdir(base_dir):
    class_dir = os.path.join(base_dir, label)


    if os.path.isdir(class_dir):
        for file_name in os.listdir(class_dir):
            img_path = os.path.join(class_dir, file_name)
            if os.path.isfile(img_path) and file_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                data.append({'image_path': img_path, 'Class': label})

df = pd.DataFrame(data)

# Save
output_csv = '/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/augmented_data.csv'
df.to_csv(output_csv, index=False)

print(f"New CSV saved to {output_csv}")

import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/augmented_data.csv')

random_rows = df.sample(n=5)

for index, row in random_rows.iterrows():
    image_path = row['image_path']
    label = row['Class']

    img = mpimg.imread(image_path)
    plt.imshow(img)
    plt.title('Label: ' + str(label))
    plt.axis('off')
    plt.show()



"""# Construct the model"""

import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import Sequential, layers
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np


augmented_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/augmented_data.csv')
train_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/train_data.csv')
val_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/val_data.csv')
test_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/test_data.csv')


combined_train_data = pd.concat([train_df, augmented_df])

train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_dataframe(
    dataframe=combined_train_data,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_dataframe(
    dataframe=val_data,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

test_generator = test_datagen.flow_from_dataframe(
    dataframe=test_df,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

# Load EfficientNetV2S pre-trained on ImageNet
base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Custom model
custom_model = Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.5),
    layers.Flatten(),
    layers.Dropout(0.3),
    layers.Dense(128, activation='elu'),
    layers.Dense(5, activation='softmax')
])

custom_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 50
validation_steps = 50

early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)

history = custom_model.fit(train_generator, epochs=epochs, validation_data=val_generator, callbacks=[early_stopping])

# Plot accuracy and loss
plt.figure(figsize=(12, 4))

# Accuracy graph
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Loss graph
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

## Test the model on unseen data to understand how the model works
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_true = test_generator.classes
y_pred = custom_model.predict(test_generator)
y_pred_classes = y_pred.argmax(axis=1)

# classification report
print(classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys()))

# confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()

"""## Check if there are so small changes in each class"""

import pandas as pd


augmented_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/augmented_data.csv')
train_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/train_data.csv')
test_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/test_data.csv')
val_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/val_data.csv')

# Filter the dataframes to only include 'AD', 'CN', and 'MCI'
train_filtered = train_df[train_df['Class'].isin(['AD', 'CN', 'MCI'])]
val_filtered = val_df[val_df['Class'].isin(['AD', 'CN', 'MCI'])]
test_filtered = test_df[test_df['Class'].isin(['AD', 'CN', 'MCI'])]
augmented_filtered = augmented_df[augmented_df['Class'].isin(['AD', 'CN', 'MCI'])]

# Save the filtered datasets to CSV files
train_filtered.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/train_AD_CN_MCI.csv', index=False)
val_filtered.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/val_AD_CN_MCI.csv', index=False)
test_filtered.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/test_AD_CN_MCI.csv', index=False)
augmented_filtered.to_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/augmented_AD_CN_MCI.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import Sequential, layers
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt

train_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/train_AD_CN_MCI.csv')
val_data = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/val_AD_CN_MCI.csv')
test_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/test_AD_CN_MCI.csv')
augmented_df = pd.read_csv('/content/drive/MyDrive/MASTER THESIS/CODE & MODELLING/EXPERIMENT/EXPERIMENT 3/Test/augmented_AD_CN_MCI.csv')

combined_train_data = pd.concat([train_df, augmented_df])

train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_dataframe(
    dataframe=combined_train_data,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_dataframe(
    dataframe=val_data,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

test_generator = test_datagen.flow_from_dataframe(
    dataframe=test_df,
    x_col='image_path',
    y_col='Class',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

# Load EfficientNetV2S pre-trained on ImageNet
base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Custom model
custom_model = Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.5),
    layers.Flatten(),
    layers.Dropout(0.3),
    layers.Dense(128, activation='elu'),
    layers.Dense(3, activation='softmax')
])

custom_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 50
validation_steps = 50

early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)

history = custom_model.fit(train_generator, epochs=epochs, validation_data=val_generator, callbacks=[early_stopping])

plt.figure(figsize=(12, 4))

# Accuracy graph
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Loss graph
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

## Test the model on unseen data to understand how the model works
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_true = test_generator.classes
y_pred = custom_model.predict(test_generator)
y_pred_classes = y_pred.argmax(axis=1)

# classification report
print(classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys()))

# confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()

"""# Efficientnet V2S on Validation"""